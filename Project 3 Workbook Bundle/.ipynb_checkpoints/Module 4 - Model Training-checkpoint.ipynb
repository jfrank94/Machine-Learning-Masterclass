{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:42px; text-align:center; margin-bottom:30px;\"><span style=\"color:SteelBlue\">Module 4:</span> Model Training</h1>\n",
    "<hr>\n",
    "\n",
    "At last, it's time to build our models! \n",
    "\n",
    "Remember, professional data scientists actually spend the bulk of their time on the 3 steps leading up to this one: \n",
    "1. Exploratory Analysis\n",
    "2. Data Cleaning\n",
    "3. Feature Engineering\n",
    "\n",
    "Again, that's because **better data beats fancier algorithms**.\n",
    "\n",
    "<br><hr id=\"toc\">\n",
    "\n",
    "### In this module...\n",
    "\n",
    "First, we'll load our analytical base table from Module 2. \n",
    "\n",
    "Then, we'll go through the essential modeling steps:\n",
    "\n",
    "1. [Split your dataset](#split)\n",
    "2. [Build model pipelines](#pipelines)\n",
    "3. [Declare hyperparameters to tune](#hyperparameters)\n",
    "4. [Fit and tune models with cross-validation](#fit-tune)\n",
    "5. [Evaluate metrics](#evaluate)\n",
    "6. [Area under ROC curve](#auroc)\n",
    "\n",
    "Finally, we'll save the best model to use in the next module. Since we've seen these steps in Project 2 already, we'll try to speed through most of them. However, we'll spend a little extra time on the last 2 steps because classification tasks require new metrics.\n",
    "\n",
    "<br><hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, let's import libraries, recruit models, and load the ABT.\n",
    "\n",
    "First, let's import the libraries that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_function for compatibility with Python 3\n",
    "from __future__ import print_function\n",
    "# NumPy for numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Pandas for DataFrames\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "\n",
    "# Matplotlib for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# display plots in the notebook\n",
    "%matplotlib inline \n",
    "\n",
    "\n",
    "# Seaborn for easier visualization\n",
    "import seaborn as sns \n",
    "# Scikit-Learn for Modeling\n",
    "import sklearn \n",
    "# Pickle for saving model files\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the algorithms we introduced in Module 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Import RandomForestClassifier and GradientBoostingClassifer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the Scikit-Learn functions and helpers we'll need for this module.\n",
    "* Remember, it's OK to just add more imports here as you need them and re-run this cell (it won't change anything else you've done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for splitting training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Function for creating model pipelines\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# For standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Helper for cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Classification metrics (added later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's read the analytical base table you saved at the end of Module 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load analytical base table from Module 2\n",
    "df = pd.read_csv('analytical_base_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"split\"></span>\n",
    "# 1. Split your dataset\n",
    "\n",
    "Just as we did in Project 2, let's start by splitting our data into separate training and test sets. \n",
    "\n",
    "<br>\n",
    "**First, separate the dataframe into separate objects for the target variable, <code style=\"color:steelblue\">y</code>, and the input features, <code style=\"color:steelblue\">X</code>.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate object for target variable\n",
    "y = df.status\n",
    "\n",
    "# Create separate object for input features\n",
    "X = df.drop('status', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After you've imported the <code style=\"color:steelblue\">train_test_split()</code> function, split <code style=\"color:steelblue\">X</code> and <code style=\"color:steelblue\">y</code> into training and test sets.**\n",
    "* Pass in the argument <code style=\"color:steelblue\">test_size=<span style=\"color:crimson\">0.2</span></code> to set aside 20% of our observations for the test set.\n",
    "* Pass in <code style=\"color:steelblue\">random_state=<span style=\"color:crimson\">1234</span></code> to set the random state for replicable results.\n",
    "* **Important:** Also pass in the argument <code style=\"color:steelblue\">stratify=<span style=\"color:crimson\">df.status</span></code> in order to make sure the target variable's classes are balanced in each subset of data! This is **stratified random sampling**.\n",
    "* Then, print the number of observations in each subset to check that it was done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11254 2814 11254 2814\n"
     ]
    }
   ],
   "source": [
    "# Split X and y into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234, stratify=df.status)\n",
    "\n",
    "# Print number of observations in X_train, X_test, y_train, and y_test\n",
    "print (len(X_train), len(X_test), len(y_train), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold\">\n",
    "<a href=\"#toc\">Back to Contents</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"pipelines\"></span>\n",
    "# 2. Build model pipelines\n",
    "\n",
    "Next, let's set up preprocessing pipelines for each of our algorithms.\n",
    "\n",
    "<br>\n",
    "**Create a single <span style=\"color:royalblue\">pipeline dictionary</span> with pipelines for each algorithm**.\n",
    "* Use the keys:\n",
    "    * <code style=\"color:crimson\">'l1'</code> for $L_1$-regularized logistic regression\n",
    "    * <code style=\"color:crimson\">'l2'</code> for $L_2$-regularized logistic regression\n",
    "    * <code style=\"color:crimson\">'rf'</code> for random forest\n",
    "    * <code style=\"color:crimson\">'gb'</code> for gradient boosted tree.\n",
    "* Each pipeline should standardize the data first.\n",
    "* Remember to set <code style=\"color:steelblue\">random_state=<span style=\"color:crimson\">123</span></code> for each algorithm to ensure replicable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline dictionary\n",
    "pipeline_dict = {'l1': make_pipeline(StandardScaler(), LogisticRegression(penalty='l1', random_state=123)), \n",
    "                                     'l2': make_pipeline(StandardScaler(), LogisticRegression(penalty='l2', random_state=123)),\n",
    "                                     'rf': make_pipeline(StandardScaler(), RandomForestClassifier(random_state=123)),\n",
    "                                     'gb': make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=123))\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold\">\n",
    "<a href=\"#toc\">Back to Contents</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"hyperparameters\"></span>\n",
    "# 3. Declare hyperparameters to tune\n",
    "\n",
    "Next, let's declare hyperparameters to tune.\n",
    "\n",
    "<br>\n",
    "**First, list the tunable hyperparameters of your $L_1$-regularized logistic regression pipeline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logisticregression': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l1', random_state=123, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 'logisticregression__C': 1.0,\n",
       " 'logisticregression__class_weight': None,\n",
       " 'logisticregression__dual': False,\n",
       " 'logisticregression__fit_intercept': True,\n",
       " 'logisticregression__intercept_scaling': 1,\n",
       " 'logisticregression__max_iter': 100,\n",
       " 'logisticregression__multi_class': 'ovr',\n",
       " 'logisticregression__n_jobs': 1,\n",
       " 'logisticregression__penalty': 'l1',\n",
       " 'logisticregression__random_state': 123,\n",
       " 'logisticregression__solver': 'liblinear',\n",
       " 'logisticregression__tol': 0.0001,\n",
       " 'logisticregression__verbose': 0,\n",
       " 'logisticregression__warm_start': False,\n",
       " 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'standardscaler__copy': True,\n",
       " 'standardscaler__with_mean': True,\n",
       " 'standardscaler__with_std': True,\n",
       " 'steps': [('standardscaler',\n",
       "   StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('logisticregression',\n",
       "   LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "             intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "             penalty='l1', random_state=123, solver='liblinear', tol=0.0001,\n",
       "             verbose=0, warm_start=False))]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List tuneable hyperparameters of our Logistic pipeline\n",
    "pipeline_dict['l1'].get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's declare the **hyperparameter grids** to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic Regression hyperparameters\n",
    "l1_hyperparameters = { 'logisticregression__C': np.linspace(1e-3, 1e3, 10)}\n",
    "l2_hyperparameters = { 'logisticregression__C': np.linspace(1e-3, 1e3, 10)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declare the hyperparameter grid for the random forest.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest hyperparameters\n",
    "rf_hyperparameters = { 'randomforest__n_estimators': [100, 200],\n",
    "                         'randomforest__max_features': ['auto', 'sqrt', 0.33]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Declare the hyperparameter grid for the boosted tree.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosted Tree hyperparameters\n",
    "gb_hyperparameters = {'gb__n_estimators': [100, 200], \n",
    "                    'gb__learning_rate': [0.05, 0.1, 0.2], \n",
    "                     'gb__max_depth': [1, 3, 5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a <code style=\"color:steelblue\">hyperparameters</code> dictionary**.\n",
    "* Use the same keys as in the <code style=\"color:steelblue\">pipelines</code> dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hyperparameters dictionary\n",
    "hp_dict = { 'l1': l1_hyperparameters, 'l2': l2_hyperparameters, 'rf': rf_hyperparameters, 'gb': gb_hyperparameters}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold\">\n",
    "<a href=\"#toc\">Back to Contents</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"fit-tune\"></span>\n",
    "# 4. Fit and tune models with cross-validation\n",
    "\n",
    "Now that we have our <code style=\"color:steelblue\">pipelines</code> and <code style=\"color:steelblue\">hyperparameters</code> dictionaries declared, we're ready to tune our models with **cross-validation**.\n",
    "\n",
    "<br>\n",
    "**Create a <code style=\"color:SteelBlue\">fitted_models</code> dictionary that includes models that have been tuned using cross-validation.**\n",
    "* The keys should be the same as those in the <code style=\"color:SteelBlue\">pipelines</code> and <code style=\"color:SteelBlue\">hyperparameters</code> dictionaries. \n",
    "* The values should be <code style=\"color:steelblue\">GridSearchCV</code> objects that have been fitted to <code style=\"color:steelblue\">X_train</code> and <code style=\"color:steelblue\">y_train</code>.\n",
    "* After fitting each model, print <code style=\"color:crimson\">'{name} has been fitted.'</code> just to track the progress.\n",
    "* (Optionally) You can set <code style=\"color:steelblue\">n_jobs=<span style=\"color:crimson\">-1</span></code> to use as many cores as available on your computer.\n",
    "\n",
    "This step can take a few minutes, so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dictionary called fitted_models\n",
    "fitted_models = {}\n",
    "\n",
    "# Loop through model pipelines, tuning each one and saving it to fitted_models\n",
    "for name, pipeline in fitted_models: \n",
    "    # Create cross-validation object from pipeline and hyperparameters\n",
    "    model = GridSearchCV(pipeline_dict, hp_dict[name], cv=10, n_jobs=-1)\n",
    "    \n",
    "    # Fit model on X_train, y_train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Store model in fitted_models[name] \n",
    "    fitted_models[name] = model\n",
    "    \n",
    "    # Print '{name} has been fitted'\n",
    "    print(name, 'has been fitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"evaluate\"></span>\n",
    "# 5. Evaluate metrics\n",
    "\n",
    "Finally, it's time to evaluate our models and pick the best one.\n",
    "\n",
    "<br>\n",
    "**First, display the <code style=\"color:steelblue\">best\\_score_</code> attribute for each fitted model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display best_score_ for each fitted model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get different numbers, check to see that you've set the <code style=\"color:steelblue\">random_state=</code> correctly for each of the models.\n",
    "\n",
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold\">\n",
    "<a href=\"#toc\">Back to Contents</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"auroc\"></span>\n",
    "# 6. Area under ROC curve\n",
    "\n",
    "**Area under ROC curve** is the most reliable metric for classification tasks.\n",
    "\n",
    "<br>\n",
    "**Add this import to the top of your Companion Workbook (good practice for keeping all your imports in one place.** \n",
    "* Remember to run the entire code cell afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classification metrics\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before presenting the idea of an ROC curve, we must first discuss what a **confusion matrix** is. \n",
    "\n",
    "Let's see an example using our $L_1$-regularized logistic regression. \n",
    "* First, let's use <code style=\"color:steelblue\">.predict()</code> to get the predicted classes directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict classes using L1-regularized logistic regression \n",
    "\n",
    "\n",
    "# Display first 5 predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, let's display the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import confusion_matrix\n",
    "\n",
    "\n",
    "# Display confusion matrix for y_test and pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we can predict a **probability** for each class using <code style=\"color:steelblue\">.predict_proba()</code>, instead of the class directly.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict PROBABILITIES using L1-regularized logistic regression\n",
    "\n",
    "\n",
    "# Get just the prediction for the positive class (1)\n",
    "\n",
    "\n",
    "# Display first 5 predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the ROC curve using the <code style=\"color:steelblue\">roc_curve()</code> function that we imported earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate ROC curve from y_test and pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can throw these into a DataFrame for convenience and look at the last 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store fpr, tpr, thresholds in DataFrame and display last 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, as you decrease the threshold, both the false positive rate **and** the true positive rate increase.\n",
    "\n",
    "We can plot the entire curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure\n",
    "\n",
    "\n",
    "# Plot ROC curve\n",
    "\n",
    "\n",
    "# Diagonal 45 degree line\n",
    "\n",
    "\n",
    "# Axes limits and labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate AUROC, use the <code style=\"color:steelblue\">auc()</code> function we imported earlier in conjunction with the <code style=\"color:steelblue\">roc_curve()</code> function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curve\n",
    "\n",
    "# Calculate AUROC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've taken a detour to dive into some of the intuition behind AUROC, let's calculate it for each of our fitted models on the test set.\n",
    "\n",
    "<br>\n",
    "**Using a <code style=\"color:SteelBlue\">for</code> loop, print the performance of each model in <code style=\"color:SteelBlue\">fitted_models</code> on the test set.**\n",
    "* Print the <code style=\"color:SteelBlue\">auc</code> of the <code style=\"color:SteelBlue\">roc_curve</code>.\n",
    "* Label the output with the name of the algorithm. For example:\n",
    "\n",
    "<pre style=\"color:crimson\">\n",
    "rf 0.991520189216\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally, save the winning <code style=\"color:steelblue\">Pipeline</code> object into a pickle file.**\n",
    "* Just save the <code style=\"color:steelblue\">Pipeline</code>, not the <code style=\"color:steelblue\">GridSearchCV</code> object.\n",
    "* **Hint:** Remember the <code style=\"color:steelblue\">.best\\_estimator_</code> attribute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save winning model as final_model.pkl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold\">\n",
    "<a href=\"#toc\">Back to Contents</a>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## Next Steps\n",
    "\n",
    "Congratulations for making through Project 3's Model Training module!\n",
    "\n",
    "As a reminder, here are a few things you did in this module:\n",
    "* You split your dataset into training and test sets.\n",
    "* You set up model pipelines and hyperparameter grids.\n",
    "* You tuned your models using cross-validation.\n",
    "* You learned about how AUROC is a more effective metric for classification than simple accuracy.\n",
    "* And finally, you saved the winning model.\n",
    "\n",
    "In the next module, <span style=\"color:royalblue\">Module 5: Project Delivery</span>, we'll see how you can go the extra mile in terms of project delivery. That includes preparing to apply your model to raw data and packaging it up into an executable script.\n",
    "\n",
    "<p style=\"text-align:center; margin: 40px 0 40px 0; font-weight:bold\">\n",
    "<a href=\"#toc\">Back to Contents</a>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
